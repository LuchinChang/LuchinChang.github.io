---
---

@inproceedings{zhang2022relyme,
  abbr={Relyme},
  title={Relyme: Improving lyric-to-melody generation by incorporating lyric-melody relationships},
  author={Zhang, Chen and Chang, Luchin and Wu, Songruoyao and Tan, Xu and Qin, Tao and Liu, Tie-Yan and Zhang, Kejun},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={1047--1056},
  year={2022},
  pdf={https://arxiv.org/pdf/2207.05688.pdf},
  code={https://github.com/microsoft/muzic/tree/main/relyme},
  website={https://luchinchang.github.io//projects/ReLyMe/},
  selected={true},
  abstract={Lyric-to-melody generation, which generates melody according
    to given lyrics, is one of the most important automatic music
    composition tasks. With the rapid development of deep learning,
    previous works address this task with end-to-end neural network
    models. However, deep learning models cannot well capture the
    strict but subtle relationships between lyrics and melodies, which
    compromises the harmony between lyrics and generated melodies.
    In this paper, we propose ReLyMe, a method that incorporates
    Relationships between Lyrics and Melodies from music theory to
    ensure the harmony between lyrics and melodies. Specifically, we
    first introduce several principles that lyrics and melodies should
    follow in terms of tone, rhythm, and structure relationships. These
    principles are then integrated into neural network lyric-to-melody
    models by adding corresponding constraints during the decoding
    process to improve the harmony between lyrics and melodies. We
    use a series of objective and subjective metrics to evaluate the generated melodies. Experiments on both English and Chinese song
    datasets show the effectiveness of ReLyMe, demonstrating the superiority of incorporating lyric-melody relationships from the music
    domain into neural lyric-to-melody generation.}
}

@inproceedings{zhang2022pdaugment,
  abbr={PDAugment},
  title={PDAugment: Data Augmentation by Pitch and Duration Adjustments for Automatic Lyrics Transcription},
  author={Zhang, Chen and Yu, Jiaxing and Chang, LuChin and Tan, Xu and Chen, Jiawei and Qin, Tao and Zhang, Kejun},
  booktitle={Ismir 2022 Hybrid Conference},
  year={2022},
  pdf={https://arxiv.org/pdf/2109.07940.pdf},
  code={https://github.com/microsoft/muzic/tree/main/pdaugment},
  website={https://luchinchang.github.io//projects/PDAugment/},
  selected={true},
  abstract={Automatic lyrics transcription (ALT), which can be regarded
    as automatic speech recognition (ASR) on singing voice, is
    an interesting and practical topic in academia and industry.
    ALT has not been well developed mainly due to the dearth
    of paired singing voice and lyrics datasets for model training.
    Considering that there is a large amount of ASR training data,
    a straightforward method is to leverage ASR data to enhance
    ALT training. However, the improvement is marginal when
    training the ALT system directly with ASR data, because
    of the gap between the singing voice and standard speech
    data which is rooted in music-specific acoustic characteristics in singing voice. In this paper, we propose PDAugment,
    a data augmentation method that adjusts pitch and duration
    of speech at syllable level under the guidance of music scores
    to help ALT training. Specifically, we adjust the pitch and
    duration of each syllable in natural speech to those of the corresponding note extracted from music scores, so as to narrow
    the gap between natural speech and singing voice. Experiments on DSing30 and Dali corpus show that the ALT system
    equipped with our PDAugment outperforms previous stateof-the-art systems by 5.9% and 18.1% WERs respectively,
    demonstrating the effectiveness of PDAugment for ALT.}
}